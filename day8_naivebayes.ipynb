{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam Dictionary:\n",
      "{'send': 3, 'us': 3, 'your': 3, 'password': 3, 'review': 1, '-': 1, 'google': 1, 'account': 1}\n",
      "\n",
      "\n",
      "Ham Dictionary:\n",
      "{'send': 1, 'us': 1, 'your': 3, 'review': 2, 'google': 2, 'password': 2, 'needs': 1, 'you': 1, 'to': 1, 'reset': 1}\n"
     ]
    }
   ],
   "source": [
    "spam_text= ['Send us your password', 'review us - google', 'Send your password', 'Send us your account password']\n",
    "\n",
    "ham_text= ['Send us your review', 'google review your password', 'Google needs you to reset your password']\n",
    "\n",
    "spam = {}\n",
    "for i in spam_text:\n",
    "    for j in i.lower().split(' '):\n",
    "        if j not in spam:\n",
    "            spam[j] = 1\n",
    "        else:\n",
    "            spam[j] += 1\n",
    "\n",
    "print(\"Spam Dictionary:\")            \n",
    "print(spam)\n",
    "print(\"\\n\")\n",
    "\n",
    "ham = {}\n",
    "for i in ham_text:\n",
    "    for j in i.lower().split(' '):\n",
    "        if j not in ham:\n",
    "            ham[j] = 1\n",
    "        else:\n",
    "            ham[j] += 1\n",
    "\n",
    "print(\"Ham Dictionary:\")\n",
    "print(ham)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_is_ham(word, is_ham):\n",
    "    if is_ham == True:\n",
    "        thing = ham.get(word)\n",
    "        if thing is None:\n",
    "            thing = 0\n",
    "        return float(thing)/ sum(ham.values())\n",
    "    if is_ham == False:\n",
    "        thing = spam.get(word)\n",
    "        if thing is None:\n",
    "            thing = 0\n",
    "        return float(thing)/ sum(spam.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'given_prob' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-cc8cef1f364c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgiven_prob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'password'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'given_prob' is not defined"
     ]
    }
   ],
   "source": [
    "given_prob('password', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def given_there_is_word(word, ham):\n",
    "    if ham:\n",
    "        return given_is_ham(word, True)*(float(2)/6) / (given_is_ham('password', False)*(float(len(spam_text))/len(spam_text+ham_text)) + given_is_ham('password', True)*(float(len(ham_text))/len(spam_text+ham_text)))\n",
    "    else:\n",
    "        return given_is_ham(word, False)*(float(4)/6) / (given_is_ham('password', False)*(float(len(spam_text))/len(spam_text+ham_text)) + given_is_ham('password', True)*(float(len(ham_text))/len(spam_text+ham_text)))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7608695652173914"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "given_there_is_word('password', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_sentence(sentence):\n",
    "    p_ham = 0\n",
    "    p_spam = 0\n",
    "    for word in sentence.lower().split(' '):\n",
    "        p_ham += np.log(given_there_is_word(word, True)) + np.log(float(2)/6)\n",
    "        p_spam += np.log(given_there_is_word(word, False)) + np.log(float(4)/6)\n",
    "    if p_ham > p_spam:\n",
    "        print('is ham', 1-p_ham/p_spam)\n",
    "    else:\n",
    "        print('is spam', 1-p_spam/p_ham)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is spam 0.42116529444513484\n"
     ]
    }
   ],
   "source": [
    "classify_sentence('Send your password google google google google google google')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "5572\n",
      "{'spam': 567, 'ham': 3612}\n",
      "0.9641\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "\n",
    "    #  tokenize our string into words\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "\n",
    "    # count up how many of each word appears in a list of words.\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit our classifier\n",
    "        Arguments:\n",
    "            X {list} -- list of document contents\n",
    "            y {list} -- correct labels\n",
    "        \"\"\"\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "\n",
    "        # Compute log class priors (the probability that any given message is spam/ham),\n",
    "        # by counting how many messages are spam/ham, \n",
    "        # dividing by the total number of messages, and taking the log.\n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 'spam')\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 'ham')\n",
    "        self.log_class_priors['spam'] = np.log(self.num_messages['spam'] / n )\n",
    "        self.log_class_priors['ham'] = np.log(self.num_messages['ham'] / n )\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "\n",
    "        # for each (document, label) pair, tokenize the document into words.\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 'spam' else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            # For each word, either add it to the vocabulary for spam/ham, \n",
    "            # if it isnâ€™t already there, and update the number of counts. \n",
    "            for word, count in counts.items():\n",
    "                # Add that word to the global vocabulary.\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "\n",
    "                self.word_counts[c][word] += count\n",
    "\n",
    "    # function to actually output the class label for new data.\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        # Given a document...\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            # We iterate through each of the words...\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "                # ... and compute log p(w_i|Spam), and sum them all up. The same will happen for Ham\n",
    "                # add Laplace smoothing\n",
    "                # https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "                log_w_given_spam = np.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = np.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    "\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "            \n",
    "            # Then we add the log class priors...\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "\n",
    "            # ... and check to see which score is bigger for that document.\n",
    "            # Whichever is larger, that is the predicted label!\n",
    "            if spam_score > ham_score:\n",
    "                result.append('spam')\n",
    "            else:\n",
    "                result.append('ham')\n",
    "        return result\n",
    "        \n",
    "\n",
    "# TODO: Fill in the below function to make a prediction, \n",
    "# your answer should match the final number in the below output (0.9641)\n",
    "if __name__ == '__main__':\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    # import/clean/label your data\n",
    "    data = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "    data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "    data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "    print(data.head())\n",
    "    tags = data[\"label\"]\n",
    "    texts = data[\"text\"]\n",
    "\n",
    "    # create texts and tags\n",
    "    X, y = texts, tags\n",
    "    print(len(X))\n",
    "    \n",
    "    # transform text into numerical vectors\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "    \n",
    "    # instantiate your SpamDetector\n",
    "    MNB = SpamDetector()\n",
    "    # fit to model, with the trained part of the dataset\n",
    "    MNB.fit(X_train.values, y_train.values)\n",
    "    print(MNB.num_messages)\n",
    "#     print(MNB.word_counts)\n",
    "    # make predictions\n",
    "    pred = MNB.predict(X_test.values)\n",
    "    true = y_test.values\n",
    "    # test for accuracy\n",
    "    accuracy = sum(1 for i in range(len(pred)) if pred[i] == true[i]) / float(len(pred))\n",
    "    print(\"{0:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9511844938980617\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import string\n",
    "import math\n",
    "import pandas as pd\n",
    "class SpamDetector(object):\n",
    "    \"\"\"Implementation of Naive Bayes for binary classification\"\"\"\n",
    "    # clean up our string by removing punctuation\n",
    "    def clean(self, s):\n",
    "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.translate(translator)\n",
    "    #  tokenize our string into words\n",
    "    def tokenize(self, text):\n",
    "        text = self.clean(text).lower()\n",
    "        return re.split(\"\\W+\", text)\n",
    "    # count up how many of each word appears in a list of words.\n",
    "    def get_word_counts(self, words):\n",
    "        word_counts = {}\n",
    "        for word in words:\n",
    "            word_counts[word] = word_counts.get(word, 0.0) + 1.0\n",
    "        return word_counts\n",
    "    def fit(self, X, Y):\n",
    "        \"\"\"Fit our classifier\n",
    "        Arguments:\n",
    "            X {list} -- list of document contents\n",
    "            y {list} -- correct labels\n",
    "        \"\"\"\n",
    "        self.num_messages = {}\n",
    "        self.log_class_priors = {}\n",
    "        self.word_counts = {}\n",
    "        self.vocab = set()\n",
    "        # Compute log class priors (the probability that any given message is spam/ham),\n",
    "        # by counting how many messages are spam/ham, \n",
    "        # dividing by the total number of messages, and taking the log.\n",
    "        n = len(X)\n",
    "        self.num_messages['spam'] = sum(1 for label in Y if label == 'spam')\n",
    "        self.num_messages['ham'] = sum(1 for label in Y if label == 'ham')\n",
    "        self.log_class_priors['spam'] = np.log(self.num_messages['spam'] / n )\n",
    "        self.log_class_priors['ham'] = np.log(self.num_messages['ham'] / n )\n",
    "        self.word_counts['spam'] = {}\n",
    "        self.word_counts['ham'] = {}\n",
    "        # for each (document, label) pair, tokenize the document into words.\n",
    "        for x, y in zip(X, Y):\n",
    "            c = 'spam' if y == 'spam' else 'ham'\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            # For each word, either add it to the vocabulary for spam/ham, \n",
    "            # if it isn't already there, and update the number of counts. \n",
    "            for word, count in counts.items():\n",
    "                # Add that word to the global vocabulary.\n",
    "                if word not in self.vocab:\n",
    "                    self.vocab.add(word)\n",
    "                if word not in self.word_counts[c]:\n",
    "                    self.word_counts[c][word] = 0.0\n",
    "                self.word_counts[c][word] += count\n",
    "    # function to actually output the class label for new data.\n",
    "    def predict(self, X):\n",
    "        result = []\n",
    "        # Given a document...\n",
    "        for x in X:\n",
    "            counts = self.get_word_counts(self.tokenize(x))\n",
    "            spam_score = 0\n",
    "            ham_score = 0\n",
    "            # We iterate through each of the words...\n",
    "            for word, _ in counts.items():\n",
    "                if word not in self.vocab: continue\n",
    "                # ... and compute log p(w_i|Spam), and sum them all up. The same will happen for Ham\n",
    "                # add Laplace smoothing\n",
    "                # https://medium.com/syncedreview/applying-multinomial-naive-bayes-to-nlp-problems-a-practical-explanation-4f5271768ebf\n",
    "                log_w_given_spam = np.log( (self.word_counts['spam'].get(word, 0.0) + 1) / (self.num_messages['spam'] + len(self.vocab)) )\n",
    "                log_w_given_ham = np.log( (self.word_counts['ham'].get(word, 0.0) + 1) / (self.num_messages['ham'] + len(self.vocab)) )\n",
    "                spam_score += log_w_given_spam\n",
    "                ham_score += log_w_given_ham\n",
    "            # Then we add the log class priors...\n",
    "            spam_score += self.log_class_priors['spam']\n",
    "            ham_score += self.log_class_priors['ham']\n",
    "            # ... and check to see which score is bigger for that document.\n",
    "            # Whichever is larger, that is the predicted label!\n",
    "            if spam_score > ham_score:\n",
    "                result.append('spam')\n",
    "            else:\n",
    "                result.append('ham')\n",
    "        return result\n",
    "# TODO: Fill in the below function to make a prediction, \n",
    "# your answer should match the final number in the below output (0.9641)\n",
    "if __name__ == '__main__':\n",
    "    sd = SpamDetector()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=19)\n",
    "    sd.fit(X_train, y_train)\n",
    "    predictions = np.array(sd.predict(X_test))\n",
    "    print(np.mean(predictions == y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  Go until jurong point, crazy.. Available only ...\n",
      "1   ham                      Ok lar... Joking wif u oni...\n",
      "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3   ham  U dun say so early hor... U c already then say...\n",
      "4   ham  Nah I don't think he goes to usf, he lives aro...\n",
      "  (0, 3286)\t1\n",
      "  (0, 4747)\t2\n",
      "  (0, 1896)\t1\n",
      "  (0, 875)\t2\n",
      "  (0, 6599)\t2\n",
      "  (0, 801)\t1\n",
      "  (0, 5258)\t1\n",
      "  (0, 7209)\t3\n",
      "  (0, 1559)\t1\n",
      "  (0, 913)\t1\n",
      "  (0, 6623)\t3\n",
      "  (0, 1050)\t1\n",
      "  (0, 5980)\t1\n",
      "  (0, 3530)\t1\n",
      "  (0, 919)\t1\n",
      "  (0, 802)\t1\n",
      "  (0, 819)\t1\n",
      "  (0, 5712)\t1\n",
      "  (0, 6727)\t1\n",
      "  (0, 2112)\t1\n",
      "  (0, 5065)\t2\n",
      "  (0, 7373)\t1\n",
      "  (0, 4176)\t2\n",
      "  (0, 1535)\t2\n",
      "  (0, 6604)\t1\n",
      "  :\t:\n",
      "  (4176, 4747)\t1\n",
      "  (4176, 3252)\t1\n",
      "  (4176, 3416)\t1\n",
      "  (4176, 2304)\t1\n",
      "  (4176, 6638)\t1\n",
      "  (4176, 4450)\t1\n",
      "  (4176, 7163)\t1\n",
      "  (4176, 4219)\t1\n",
      "  (4176, 1590)\t1\n",
      "  (4176, 3439)\t1\n",
      "  (4176, 4833)\t1\n",
      "  (4176, 4894)\t1\n",
      "  (4177, 3647)\t1\n",
      "  (4177, 3252)\t1\n",
      "  (4177, 6074)\t1\n",
      "  (4177, 4125)\t1\n",
      "  (4177, 3162)\t1\n",
      "  (4177, 4766)\t1\n",
      "  (4177, 1848)\t1\n",
      "  (4177, 5232)\t1\n",
      "  (4177, 6953)\t1\n",
      "  (4178, 4274)\t1\n",
      "  (4178, 7295)\t1\n",
      "  (4178, 6055)\t1\n",
      "  (4178, 1695)\t1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9856424982053122"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "# from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "# Prepare the dataset\n",
    "data = pd.read_csv('spam.csv',encoding='latin-1')\n",
    "data = data.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], axis=1)\n",
    "data = data.rename(columns={\"v1\":'label', \"v2\":'text'})\n",
    "print(data.head())\n",
    "tags = data[\"label\"]\n",
    "texts = data[\"text\"]\n",
    "\n",
    "# create texts and tags\n",
    "X, y = texts, tags\n",
    "\n",
    "# split the data into train vs test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# transform text into numerical vectors\n",
    "vectorizer = CountVectorizer()\n",
    "X_train_dtm = vectorizer.fit_transform(X_train)\n",
    "print(X_train_dtm)\n",
    "\n",
    "# instantiate Multinomial Naive Bayes model\n",
    "nb = MultinomialNB()\n",
    "# fit to model, with the trained part of the dataset\n",
    "nb.fit(X_train_dtm, y_train)\n",
    "X_test_dtm = vectorizer.transform(X_test)\n",
    "# make prediction\n",
    "y_pred_class = nb.predict(X_test_dtm)\n",
    "# test accurarcy of prediction\n",
    "metrics.accuracy_score(y_test, y_pred_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.7.6 (default, Dec 30 2019, 19:38:28) \n",
      "[Clang 11.0.0 (clang-1100.0.33.16)]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
